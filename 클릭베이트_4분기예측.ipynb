{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqg8JL2sLLFfqrgXZVJWH5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/changmin-jen/clickbait-detection/blob/main/%ED%81%B4%EB%A6%AD%EB%B2%A0%EC%9D%B4%ED%8A%B8_4%EB%B6%84%EA%B8%B0%EC%98%88%EC%B8%A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YHmPJMDZiaw8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d801a380-61b1-4205-acc0-4b571770c478"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Clickbait Pair-Fusion (Title/Thumb + STT/Keyframe, Var-Length)\n",
        "# End-to-end Colab script (Top-k pooling for all branches)\n",
        "# ============================================================\n",
        "\n",
        "# -------- 0) 기본 설정 & 드라이브 마운트 --------\n",
        "import os, random, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "ROOT = \"/content/drive/MyDrive/clickbait_data\"\n",
        "EMB_NPZ = f\"{ROOT}/embeddings_sbert_dot_v1.npz\"\n",
        "LABEL_XLSX = f\"{ROOT}/clickbait_enriched_caps_QWEN_thumbcaps_qwen.xlsx\"\n",
        "\n",
        "ID_COL = \"id\"; LABEL_COL = \"label\"; SPLIT_COL = \"split\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터\n",
        "BATCH_TRAIN = 64\n",
        "BATCH_EVAL  = 256\n",
        "LR = 3e-4\n",
        "EPOCHS = 50\n",
        "MU = 1.5 #fuse가중비율\n",
        "BR_W = 0.5   # 브랜치 합에 곱할 가중\n",
        "WD = 1e-3\n",
        "HIDDEN = 64\n",
        "DROPOUT = 0.4\n",
        "LABEL_SMOOTH = 0.01\n",
        "SEED = 42"
      ],
      "metadata": {
        "id": "XwEXw1WCod4Z"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- 1) 시드 고정 --------\n",
        "import numpy as np, torch, pandas as pd\n",
        "def set_seed(seed=SEED):\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "set_seed(SEED)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAIHiyfaod2M",
        "outputId": "1c6c1eda-e700-45f1-fcea-cb6fd8cdd054"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- 2) NPZ 로딩 --------\n",
        "npz = np.load(EMB_NPZ, allow_pickle=True)\n",
        "print(\"NPZ keys ->\", list(npz.files))\n",
        "\n",
        "def npz_get(npz_obj, candidates):\n",
        "    keys = list(npz_obj.files)\n",
        "    for c in candidates:\n",
        "        if c in keys: return npz_obj[c]\n",
        "        if f\"{c}.npy\" in keys: return npz_obj[f\"{c}.npy\"]\n",
        "    for c in candidates:\n",
        "        m = [k for k in keys if c in k]\n",
        "        if len(m) == 1: return npz_obj[m[0]]\n",
        "    raise KeyError(f\"NPZ에 {candidates} 중 일치 키 없음. 존재키={keys}\")\n",
        "\n",
        "emb_title = npz_get(npz, [\"emb_title\",\"title\"])\n",
        "emb_thumb = npz_get(npz, [\"emb_thumb\",\"thumb\"])\n",
        "emb_stt   = npz_get(npz, [\"emb_stt\",\"stt\"])\n",
        "emb_kf    = npz_get(npz, [\"emb_kf\",\"kf\"])\n",
        "index_ids = npz_get(npz, [\"index\"])\n",
        "\n",
        "def to_2d(arr):\n",
        "    if isinstance(arr, np.ndarray) and arr.ndim == 2: return arr\n",
        "    if isinstance(arr, np.ndarray) and arr.dtype == object: return arr\n",
        "    if isinstance(arr, np.ndarray) and arr.ndim == 1 and arr.dtype == object:\n",
        "        return np.stack([np.asarray(v).reshape(-1) for v in arr], axis=0)\n",
        "    if isinstance(arr, np.ndarray) and arr.ndim == 1: return arr.reshape(1,-1)\n",
        "    return arr\n",
        "\n",
        "emb_title = to_2d(emb_title)\n",
        "emb_thumb = to_2d(emb_thumb)\n",
        "print(\"shapes:\", emb_title.shape, (len(emb_stt),\"object_seq\"),\n",
        "      emb_thumb.shape, (len(emb_kf),\"object_seq\"), \"index_len:\", len(index_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmFNIExqodzz",
        "outputId": "0de64d05-6d80-4122-a3d3-981a74c931f7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NPZ keys -> ['emb_title', 'emb_thumb', 'emb_stt', 'emb_kf', 'index', 'title_col', 'stt_col', 'kf_col', 'thumb_col']\n",
            "shapes: (398, 768) (398, 'object_seq') (398, 768) (398, 'object_seq') index_len: 398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- 3) 라벨 로딩 & 인덱스 매핑(위치 기반) --------\n",
        "df = pd.read_excel(LABEL_XLSX)\n",
        "df[ID_COL] = df[ID_COL].astype(str).str.strip()\n",
        "df[LABEL_COL] = df[LABEL_COL].astype(str).str.strip().str.lower()\n",
        "df[SPLIT_COL] = df[SPLIT_COL].astype(str).str.strip().str.lower().replace({\"val\":\"valid\"})\n",
        "\n",
        "N = emb_title.shape[0]\n",
        "assert emb_thumb.shape[0]==N and len(emb_stt)==N and len(emb_kf)==N\n",
        "assert len(df)==N, f\"라벨({len(df)}) != 임베딩({N})\"\n",
        "\n",
        "df = df.reset_index(drop=True).copy()\n",
        "df[\"emb_idx\"] = np.arange(N, dtype=int)\n",
        "df[\"y\"] = (df[LABEL_COL] == \"clickbait\").astype(\"float32\")\n",
        "\n",
        "df_tr = df[df[SPLIT_COL]==\"train\"].reset_index(drop=True)\n",
        "df_va = df[df[SPLIT_COL]==\"valid\"].reset_index(drop=True)\n",
        "df_te = df[df[SPLIT_COL]==\"test\"].reset_index(drop=True)\n",
        "print(\"split sizes:\", len(df_tr), len(df_va), len(df_te))\n",
        "\n",
        "# 클래스 가중치(선택)\n",
        "pos = float(df_tr[\"y\"].sum()); neg = float((1 - df_tr[\"y\"]).sum())\n",
        "pos_weight = torch.tensor([(neg / max(pos, 1.0))], device=device)\n",
        "print(\"pos_weight:\", pos_weight.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1LeNC8fojbk",
        "outputId": "38b1db28-f72d-472c-a547-7114f865a18b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "split sizes: 200 98 100\n",
            "pos_weight: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- 4) Dataset / DataLoader (정규화 포함) --------\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# 고정 길이 L2 정규화 적용\n",
        "def l2n(t):\n",
        "    return t / (t.norm(dim=1, keepdim=True) + 1e-8)  # 두 벡터를 단위벡터로 만들어 크기가 같게한다(크기가 같은 벡터의 내적은 코사인유사도 이기때문)\n",
        "\n",
        "T_title = l2n(torch.from_numpy(emb_title).float())\n",
        "T_thumb = l2n(torch.from_numpy(emb_thumb).float())\n",
        "\n",
        "# 시퀀스 정리 함수\n",
        "def get_seq(obj_arr, i):\n",
        "    x = obj_arr[i]\n",
        "    x = torch.as_tensor(x, dtype=torch.float32)\n",
        "    if x.ndim == 1:\n",
        "        x = x.unsqueeze(0)\n",
        "    if x.size(0) == 0:  # 빈 시퀀스 방어\n",
        "        x = torch.zeros(1, x.size(1))\n",
        "    # 시퀀스 내부 L2 정규화\n",
        "    x = x / (x.norm(dim=1, keepdim=True) + 1e-8)\n",
        "    return x\n",
        "\n",
        "class PairFusionVarLenDS(Dataset):\n",
        "    # 데이터셋: 제목, 썸네일, STT, 키프레임\n",
        "    def __init__(self, frame):\n",
        "        self.idx = frame[\"emb_idx\"].to_numpy().astype(int)\n",
        "        self.y = frame[\"y\"].to_numpy().astype(np.float32)\n",
        "    def __len__(self):\n",
        "        return len(self.idx)\n",
        "    def __getitem__(self, i):\n",
        "        j = int(self.idx[i])\n",
        "        return {\n",
        "            \"t\": T_title[j], \"th\": T_thumb[j],\n",
        "            \"s\": get_seq(emb_stt, j), \"k\": get_seq(emb_kf, j),\n",
        "            \"y\": torch.tensor(self.y[i])\n",
        "        }\n",
        "\n",
        "def pad_and_mask(seqs):\n",
        "    lens = [x.size(0) for x in seqs]\n",
        "    L = max(lens)\n",
        "    d = seqs[0].size(1)\n",
        "    B = len(seqs)\n",
        "    out = torch.zeros(B, L, d)\n",
        "    mask = torch.zeros(B, L, dtype=torch.bool)\n",
        "    for i, x in enumerate(seqs):\n",
        "        out[i, :x.size(0)] = x\n",
        "        mask[i, :x.size(0)] = True\n",
        "    return out, mask\n",
        "\n",
        "def collate_fn(batch):\n",
        "    t = torch.stack([b[\"t\"] for b in batch], 0)\n",
        "    th = torch.stack([b[\"th\"] for b in batch], 0)\n",
        "    s_p, s_m = pad_and_mask([b[\"s\"] for b in batch])\n",
        "    k_p, k_m = pad_and_mask([b[\"k\"] for b in batch])\n",
        "    y = torch.stack([b[\"y\"] for b in batch], 0)\n",
        "    return {\"t\": t, \"th\": th, \"s\": s_p, \"s_mask\": s_m, \"k\": k_p, \"k_mask\": k_m, \"y\": y}\n",
        "\n",
        "# 데이터로더 정의\n",
        "train_dl = DataLoader(PairFusionVarLenDS(df_tr), batch_size=BATCH_TRAIN, shuffle=True, num_workers=2, pin_memory=True, collate_fn=collate_fn)\n",
        "val_dl = DataLoader(PairFusionVarLenDS(df_va), batch_size=BATCH_EVAL, shuffle=False, num_workers=2, pin_memory=True, collate_fn=collate_fn)\n",
        "test_dl = DataLoader(PairFusionVarLenDS(df_te), batch_size=BATCH_EVAL, shuffle=False, num_workers=2, pin_memory=True, collate_fn=collate_fn)\n",
        "\n"
      ],
      "metadata": {
        "id": "cEuWvM-Zodr1"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- 5) 모델  --------\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "\n",
        "\n",
        "def pair_features(h1,h2): return torch.cat([h1,h2,torch.abs(h1-h2),h1*h2], dim=-1)\n",
        "\n",
        "class SeqPool(nn.Module):\n",
        "    def __init__(self, d, mode=\"attn\", k=3, tau=0.5):\n",
        "        super().__init__()\n",
        "        self.mode, self.k, self.tau = mode, k, tau\n",
        "\n",
        "    def forward(self, X, mask, q=None):\n",
        "        if self.mode == \"mean\":\n",
        "            pooled = (X * mask.unsqueeze(-1)).sum(1) / (mask.sum(1, keepdim=True) + 1e-8)\n",
        "            return pooled, None  # alpha/idx 없음\n",
        "\n",
        "        if self.mode == \"attn\":\n",
        "            assert q is not None\n",
        "            score = F.cosine_similarity(X, q.unsqueeze(1), dim=-1).masked_fill(~mask, -1e9)\n",
        "            alpha = F.softmax(score / self.tau, dim=1)\n",
        "            pooled = (X * alpha.unsqueeze(-1)).sum(1)\n",
        "            return pooled, alpha  # 가중치 반환\n",
        "\n",
        "        if self.mode == \"topk\":\n",
        "            score = X.norm(dim=-1).masked_fill(~mask, -1e9)\n",
        "            k = min(self.k, X.size(1))\n",
        "            idx = torch.topk(score, k, dim=1).indices\n",
        "            b = torch.arange(X.size(0), device=X.device)[:, None]\n",
        "            picked = X[b, idx]\n",
        "            pooled = picked.mean(1)\n",
        "            return pooled, idx  # 선택 인덱스 반환(해석용)\n",
        "\n",
        "        if self.mode == \"topk_sim\":\n",
        "            assert q is not None\n",
        "            score = F.cosine_similarity(X, q.unsqueeze(1), dim=-1).masked_fill(~mask, -1e9)\n",
        "            k = min(self.k, X.size(1))\n",
        "            idx = torch.topk(score, k, dim=1).indices\n",
        "            b = torch.arange(X.size(0), device=X.device)[:, None]\n",
        "            picked = X[b, idx]\n",
        "            pooled = picked.mean(1)\n",
        "            return pooled, score  # 점수 전체(가중치 시각화용)\n",
        "\n",
        "        raise ValueError(self.mode)\n",
        "\n",
        "\n",
        "class BranchMLP(nn.Module):\n",
        "    def __init__(self, d4, hidden=HIDDEN, dropout=DROPOUT, prior_p=None):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d4, hidden), nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden, 1)\n",
        "        )\n",
        "        if prior_p is not None:\n",
        "            b = np.log(prior_p/(1-prior_p + 1e-8))\n",
        "            with torch.no_grad():\n",
        "                self.net[-1].bias.fill_(b)\n",
        "    def forward(self, z):\n",
        "        return self.net(z).squeeze(-1)\n",
        "\n",
        "\n",
        "class ClickbaitPairFusionVar(nn.Module):\n",
        "    def __init__(self, d):\n",
        "        super().__init__()\n",
        "        self.pool_s  = SeqPool(d, mode=\"attn\", tau=0.5)  # STT\n",
        "        self.pool_k  = SeqPool(d, mode=\"attn\", tau=0.5)  # Keyframe\n",
        "        d4 = d * 4\n",
        "        self.pre_norm  = nn.LayerNorm(d)\n",
        "        self.norm_pair = nn.LayerNorm(d4)\n",
        "        self.b_TK  = BranchMLP(d4)\n",
        "        self.b_TS  = BranchMLP(d4)\n",
        "        self.b_ThK = BranchMLP(d4)\n",
        "        self.b_ThS = BranchMLP(d4)\n",
        "        self.norm_z  = nn.LayerNorm(d4)\n",
        "        self.att_w   = nn.Linear(d4, 1, bias=False)\n",
        "        self.fuse_out = nn.Linear(d4, 1)\n",
        "\n",
        "    def forward(self, t, th, s_pad, s_mask, k_pad, k_mask):\n",
        "        # ---- 1) 브랜치별 쿼리로 풀링 결과 4개 생성 ----\n",
        "        if self.pool_s.mode in (\"topk_sim\", \"attn\"):\n",
        "            # STT\n",
        "            s_by_t,  _  = self.pool_s(s_pad, s_mask, q=t)     # TS용\n",
        "            s_by_th, _  = self.pool_s(s_pad, s_mask, q=th)    # ThS용\n",
        "        else:\n",
        "            s_once, _ = self.pool_s(s_pad, s_mask)\n",
        "            s_by_t = s_by_th = s_once\n",
        "\n",
        "        if self.pool_k.mode in (\"topk_sim\", \"attn\"):\n",
        "            # Keyframe\n",
        "            k_by_t,  _  = self.pool_k(k_pad, k_mask, q=t)     # TK용\n",
        "            k_by_th, _  = self.pool_k(k_pad, k_mask, q=th)    # ThK용\n",
        "        else:\n",
        "            k_once, _ = self.pool_k(k_pad, k_mask)\n",
        "            k_by_t = k_by_th = k_once\n",
        "\n",
        "        # ---- 2) 정규화 ----\n",
        "        t      = self.pre_norm(t)\n",
        "        th     = self.pre_norm(th)\n",
        "        s_by_t = self.pre_norm(s_by_t)\n",
        "        s_by_th= self.pre_norm(s_by_th)\n",
        "        k_by_t = self.pre_norm(k_by_t)\n",
        "        k_by_th= self.pre_norm(k_by_th)\n",
        "\n",
        "        # ---- 3) 브랜치별 pair 특징 만들기 (각각 맞는 풀링 결과 사용) ----\n",
        "        z_TK  = self.norm_pair(pair_features(t,  k_by_t)) # TK: (title, keyframe-by-title\n",
        "        z_TS  = self.norm_pair(pair_features(t,  s_by_t)) # TS: (title, stt-by-title)\n",
        "        z_ThK = self.norm_pair(pair_features(th, k_by_th)) # ThK: (thumbnail, keyframe-by-thumbnail)\n",
        "        z_ThS = self.norm_pair(pair_features(th, s_by_th)) # ThS: (thumbnail, stt-by-thumbnail)\n",
        "\n",
        "        # ---- 4) 브랜치별 1차 로짓 ----\n",
        "        s_TK  = self.b_TK(z_TK)\n",
        "        s_TS  = self.b_TS(z_TS)\n",
        "        s_ThK = self.b_ThK(z_ThK)\n",
        "        s_ThS = self.b_ThS(z_ThS)\n",
        "\n",
        "        # ---- 5) 브랜치 attention 융합 (4개 모두 포함) ----\n",
        "        Z = torch.stack([z_TK, z_TS, z_ThK, z_ThS], dim=1)   # (B,4,4d)\n",
        "        Z = self.norm_z(Z)\n",
        "        a = F.softmax(self.att_w(Z).squeeze(-1), dim=1)      # (B,4)\n",
        "        Zf = (Z * a.unsqueeze(-1)).sum(1)                    # (B,4d)\n",
        "        s_fuse = self.fuse_out(Zf).squeeze(-1)               # (B,)\n",
        "\n",
        "        return {\n",
        "            \"branch_logits\": torch.stack([s_TK, s_TS, s_ThK, s_ThS], 1),  # (B,4)\n",
        "            \"fuse_logit\": s_fuse,                                         # (B,)\n",
        "            \"alpha\": a,                                                   # (B,4)\n",
        "            \"Z\": Z,\n",
        "            # 디버깅/해석용 (원하면 꺼도 됨)\n",
        "            \"s_by_t\": s_by_t, \"s_by_th\": s_by_th,\n",
        "            \"k_by_t\": k_by_t, \"k_by_th\": k_by_th,\n",
        "        }\n",
        "\n",
        "d = T_title.shape[1]\n",
        "model = ClickbaitPairFusionVar(d).to(device)\n",
        "print(\"params(M):\", sum(p.numel() for p in model.parameters())/1e6)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xj5qctwJodkZ",
        "outputId": "76391fa0-e240-415c-e117-e69f763f47cf"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "params(M): 0.806917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- 6) 학습/평가 --------\n",
        "bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "def run_epoch(dl, train=True,return_preds=False):\n",
        "    model.train(train)\n",
        "    tot, n = 0.0, 0\n",
        "    all_p, all_y = [], []\n",
        "    ctx = torch.enable_grad() if train else torch.no_grad()\n",
        "\n",
        "    with ctx:\n",
        "        for B in dl:\n",
        "            y = B[\"y\"].to(device)\n",
        "            y_sm = y * (1 - LABEL_SMOOTH) + 0.5 * LABEL_SMOOTH\n",
        "\n",
        "            # -------- 모델 forward (6개 인자) --------\n",
        "            out = model(\n",
        "                B[\"t\"].to(device),\n",
        "                B[\"th\"].to(device),\n",
        "                B[\"s\"].to(device),       # s_pad\n",
        "                B[\"s_mask\"].to(device),  # s_mask\n",
        "                B[\"k\"].to(device),       # k_pad\n",
        "                B[\"k_mask\"].to(device)   # k_mask\n",
        "            )\n",
        "\n",
        "            branch_logits = out[\"branch_logits\"]\n",
        "            fuse_logit    = out[\"fuse_logit\"]\n",
        "\n",
        "            # -------- Loss 계산 --------\n",
        "            branch_loss = sum(bce(bl, y_sm) for bl in branch_logits.T)/4\n",
        "            loss = BR_W * branch_loss + MU * bce(fuse_logit, y_sm)\n",
        "\n",
        "            # -------- Train step --------\n",
        "            if train:\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # -------- Metric --------\n",
        "            bs = y.size(0)\n",
        "            tot += float(loss) * bs\n",
        "            n += bs\n",
        "\n",
        "            p = torch.sigmoid(fuse_logit).detach().cpu().numpy()\n",
        "            all_p.append(p)\n",
        "            all_y.append(y.detach().cpu().numpy())\n",
        "\n",
        "    all_p = np.concatenate(all_p)\n",
        "    all_y = np.concatenate(all_y)\n",
        "    auc = roc_auc_score(all_y, all_p) if len(np.unique(all_y)) > 1 else float(\"nan\")\n",
        "    acc = accuracy_score(all_y, (all_p >= 0.5).astype(int))\n",
        "\n",
        "    if return_preds:\n",
        "        return tot/n, auc, acc, all_p, all_y\n",
        "    return tot/n, auc, acc\n",
        "\n",
        "\n",
        "# -------- 7) 모델 학습 --------\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=3)\n",
        "best_auc, best_state, no_improve = -1.0, None, 0\n",
        "\n",
        "patience=8\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    tr_loss, tr_auc, tr_acc = run_epoch(train_dl, True)\n",
        "    va_loss, va_auc, va_acc = run_epoch(val_dl, False)\n",
        "    print(f\"[{ep:02d}] train loss {tr_loss:.4f} auc {tr_auc:.3f} acc {tr_acc:.3f} | \"\n",
        "          f\"val loss {va_loss:.4f} auc {va_auc:.3f} acc {va_acc:.3f}\")\n",
        "    scheduler.step(va_auc)\n",
        "    if va_auc > best_auc:\n",
        "        best_auc = va_auc\n",
        "        best_state = {k:v.cpu() for k,v in model.state_dict().items()}\n",
        "        no_improve = 0\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= patience:\n",
        "            print(f\"Early stop at epoch {ep} (best AUC={best_auc:.3f})\")\n",
        "            break\n",
        "\n",
        "if best_state is not None:\n",
        "    model.load_state_dict({k:v.to(device) for k,v in best_state.items()})\n",
        "\n",
        "SAVE_PATH = f\"{ROOT}/pair_fusion_sbert_dot.pt\"\n",
        "torch.save(model.state_dict(), SAVE_PATH)\n",
        "print(\"saved to:\", SAVE_PATH)\n",
        "\n",
        "te_loss, te_auc, te_acc = run_epoch(test_dl, False)\n",
        "print(f\"TEST loss {te_loss:.4f} | AUC {te_auc:.3f} | ACC {te_acc:.3f}\")"
      ],
      "metadata": {
        "id": "eYB6H_mVr1bD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1f64476-c7c7-4bfc-8461-b5fe2c4e4bd7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[01] train loss 1.4740 auc 0.418 acc 0.455 | val loss 1.3657 auc 0.612 acc 0.500\n",
            "[02] train loss 1.3510 auc 0.545 acc 0.520 | val loss 1.3265 auc 0.652 acc 0.653\n",
            "[03] train loss 1.2836 auc 0.646 acc 0.600 | val loss 1.3152 auc 0.681 acc 0.592\n",
            "[04] train loss 1.2108 auc 0.773 acc 0.690 | val loss 1.2892 auc 0.702 acc 0.622\n",
            "[05] train loss 1.1519 auc 0.838 acc 0.750 | val loss 1.2725 auc 0.717 acc 0.612\n",
            "[06] train loss 1.0906 auc 0.886 acc 0.805 | val loss 1.2607 auc 0.730 acc 0.643\n",
            "[07] train loss 1.0364 auc 0.914 acc 0.830 | val loss 1.2483 auc 0.739 acc 0.633\n",
            "[08] train loss 0.9851 auc 0.938 acc 0.895 | val loss 1.2360 auc 0.750 acc 0.663\n",
            "[09] train loss 0.9423 auc 0.955 acc 0.910 | val loss 1.2241 auc 0.759 acc 0.684\n",
            "[10] train loss 0.8893 auc 0.963 acc 0.925 | val loss 1.2145 auc 0.760 acc 0.673\n",
            "[11] train loss 0.8580 auc 0.971 acc 0.900 | val loss 1.2126 auc 0.767 acc 0.684\n",
            "[12] train loss 0.8130 auc 0.978 acc 0.920 | val loss 1.2152 auc 0.776 acc 0.704\n",
            "[13] train loss 0.7725 auc 0.983 acc 0.940 | val loss 1.2242 auc 0.787 acc 0.673\n",
            "[14] train loss 0.7439 auc 0.985 acc 0.935 | val loss 1.2308 auc 0.794 acc 0.663\n",
            "[15] train loss 0.7078 auc 0.988 acc 0.935 | val loss 1.2046 auc 0.798 acc 0.684\n",
            "[16] train loss 0.6674 auc 0.989 acc 0.940 | val loss 1.1838 auc 0.804 acc 0.735\n",
            "[17] train loss 0.6428 auc 0.991 acc 0.945 | val loss 1.1822 auc 0.807 acc 0.786\n",
            "[18] train loss 0.6105 auc 0.993 acc 0.950 | val loss 1.1836 auc 0.810 acc 0.735\n",
            "[19] train loss 0.5806 auc 0.993 acc 0.960 | val loss 1.1921 auc 0.816 acc 0.673\n",
            "[20] train loss 0.5524 auc 0.995 acc 0.965 | val loss 1.1915 auc 0.817 acc 0.663\n",
            "[21] train loss 0.5267 auc 0.995 acc 0.965 | val loss 1.1960 auc 0.818 acc 0.663\n",
            "[22] train loss 0.5055 auc 0.996 acc 0.975 | val loss 1.1880 auc 0.820 acc 0.673\n",
            "[23] train loss 0.4833 auc 0.998 acc 0.975 | val loss 1.1849 auc 0.824 acc 0.684\n",
            "[24] train loss 0.4633 auc 0.998 acc 0.975 | val loss 1.1839 auc 0.825 acc 0.704\n",
            "[25] train loss 0.4467 auc 0.999 acc 0.975 | val loss 1.1700 auc 0.828 acc 0.724\n",
            "[26] train loss 0.4254 auc 0.999 acc 0.985 | val loss 1.1593 auc 0.831 acc 0.755\n",
            "[27] train loss 0.4123 auc 0.999 acc 0.980 | val loss 1.1602 auc 0.831 acc 0.765\n",
            "[28] train loss 0.3940 auc 1.000 acc 0.985 | val loss 1.1761 auc 0.833 acc 0.735\n",
            "[29] train loss 0.3776 auc 1.000 acc 0.995 | val loss 1.1752 auc 0.836 acc 0.735\n",
            "[30] train loss 0.3636 auc 1.000 acc 0.995 | val loss 1.1682 auc 0.839 acc 0.745\n",
            "[31] train loss 0.3470 auc 1.000 acc 0.995 | val loss 1.1683 auc 0.841 acc 0.735\n",
            "[32] train loss 0.3346 auc 1.000 acc 0.995 | val loss 1.1699 auc 0.842 acc 0.745\n",
            "[33] train loss 0.3244 auc 1.000 acc 0.995 | val loss 1.1731 auc 0.845 acc 0.745\n",
            "[34] train loss 0.3103 auc 1.000 acc 0.995 | val loss 1.1893 auc 0.847 acc 0.745\n",
            "[35] train loss 0.3025 auc 1.000 acc 0.995 | val loss 1.2029 auc 0.848 acc 0.714\n",
            "[36] train loss 0.2914 auc 1.000 acc 0.990 | val loss 1.2053 auc 0.850 acc 0.714\n",
            "[37] train loss 0.2804 auc 1.000 acc 0.995 | val loss 1.2001 auc 0.849 acc 0.745\n",
            "[38] train loss 0.2694 auc 1.000 acc 1.000 | val loss 1.2060 auc 0.850 acc 0.755\n",
            "[39] train loss 0.2600 auc 1.000 acc 1.000 | val loss 1.2057 auc 0.848 acc 0.755\n",
            "[40] train loss 0.2513 auc 1.000 acc 1.000 | val loss 1.2019 auc 0.850 acc 0.755\n",
            "[41] train loss 0.2425 auc 1.000 acc 1.000 | val loss 1.2036 auc 0.850 acc 0.755\n",
            "[42] train loss 0.2347 auc 1.000 acc 1.000 | val loss 1.1973 auc 0.852 acc 0.755\n",
            "[43] train loss 0.2288 auc 1.000 acc 1.000 | val loss 1.1858 auc 0.853 acc 0.786\n",
            "[44] train loss 0.2214 auc 1.000 acc 1.000 | val loss 1.1843 auc 0.853 acc 0.776\n",
            "[45] train loss 0.2161 auc 1.000 acc 1.000 | val loss 1.1856 auc 0.855 acc 0.776\n",
            "[46] train loss 0.2084 auc 1.000 acc 1.000 | val loss 1.1865 auc 0.854 acc 0.786\n",
            "[47] train loss 0.2012 auc 1.000 acc 1.000 | val loss 1.1922 auc 0.855 acc 0.776\n",
            "[48] train loss 0.1952 auc 1.000 acc 1.000 | val loss 1.2052 auc 0.854 acc 0.745\n",
            "[49] train loss 0.1910 auc 1.000 acc 1.000 | val loss 1.2115 auc 0.855 acc 0.745\n",
            "[50] train loss 0.1858 auc 1.000 acc 1.000 | val loss 1.2099 auc 0.857 acc 0.745\n",
            "saved to: /content/drive/MyDrive/clickbait_data/pair_fusion_sbert_dot.pt\n",
            "TEST loss 1.4916 | AUC 0.744 | ACC 0.660\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "\n",
        "def find_best_f1_threshold(p, y, step=0.01):\n",
        "    best_t, best_f1 = 0.5, -1\n",
        "    best_stats = None\n",
        "\n",
        "    # threshold 후보들\n",
        "    ts = np.arange(0.0, 1.0 + 1e-9, step)\n",
        "\n",
        "    for t in ts:\n",
        "        pred = (p >= t).astype(int)\n",
        "        f1 = f1_score(y, pred, zero_division=0)\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_t = t\n",
        "            best_stats = {\n",
        "                \"acc\": accuracy_score(y, pred),\n",
        "                \"prec\": precision_score(y, pred, zero_division=0),\n",
        "                \"rec\": recall_score(y, pred, zero_division=0),\n",
        "                \"f1\": f1\n",
        "            }\n",
        "    return best_t, best_stats\n"
      ],
      "metadata": {
        "id": "F2TtWzkxe5mZ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yFtAjR3oocRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# best_state 로드 이후\n",
        "if best_state is not None:\n",
        "    model.load_state_dict({k:v.to(device) for k,v in best_state.items()})\n",
        "\n",
        "# 1) val 예측 뽑기\n",
        "va_loss, va_auc, va_acc, va_p, va_y = run_epoch(val_dl, train=False, return_preds=True)\n",
        "\n",
        "# 2) val에서 best F1 threshold 찾기\n",
        "best_t, va_stats = find_best_f1_threshold(va_p, va_y, step=0.01)\n",
        "print(f\"[VAL] best threshold={best_t:.2f} | \"\n",
        "      f\"ACC={va_stats['acc']:.3f} P={va_stats['prec']:.3f} R={va_stats['rec']:.3f} F1={va_stats['f1']:.3f} | AUC={va_auc:.3f}\")\n",
        "\n",
        "# 3) test 예측 뽑고, 같은 threshold로 평가\n",
        "te_loss, te_auc, te_acc, te_p, te_y = run_epoch(test_dl, train=False, return_preds=True)\n",
        "te_pred = (te_p >= best_t).astype(int)\n",
        "\n",
        "te_acc2 = accuracy_score(te_y, te_pred)\n",
        "te_prec = precision_score(te_y, te_pred, zero_division=0)\n",
        "te_rec  = recall_score(te_y, te_pred, zero_division=0)\n",
        "te_f1   = f1_score(te_y, te_pred, zero_division=0)\n",
        "\n",
        "print(f\"[TEST@thr={best_t:.2f}] loss {te_loss:.4f} | AUC {te_auc:.3f} | \"\n",
        "      f\"ACC {te_acc2:.3f} P {te_prec:.3f} R {te_rec:.3f} F1 {te_f1:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJ7byfTbe8jJ",
        "outputId": "d054a585-b910-4c7c-982c-217e53ebc533"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VAL] best threshold=0.57 | ACC=0.786 P=0.745 R=0.854 F1=0.796 | AUC=0.853\n",
            "[TEST@thr=0.57] loss 1.5062 | AUC 0.740 | ACC 0.680 P 0.680 R 0.680 F1 0.680\n"
          ]
        }
      ]
    }
  ]
}